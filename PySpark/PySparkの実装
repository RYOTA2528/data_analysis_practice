## PySparkの準備
from pyspark.sql import SparkSession

## SparkSessionを作る
spark = SparkSession.builder.appName('Data_wrangling').getOrCreate()

## ファイルの読み込み(",区切りでheaderはそのまま使う")
# .option("inferSchema", "True") で自動でスキーマ作成
df = spark.read.format("csv") \
        .option("inferSchema", "True") \
        .opion("header", "True") \
        .option("sep", ",") \
        .load("../data/Onleine Retail.csv")
df
df.show() #実際のデータを表示

#各カラムの型・nullable = trueがわかる
# TRUE　でnull許容
df.printSchema() 

type(df) 


## スキーマ指定の型を自分で決めて取り込む
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType

schema = StructType([
    StructField("InvoiceNo", StringType(), False),
    StructField("StockCode", StringType(), False),
    StructField("Description", StringType(), False),
    StructField("Quantity", IntegerType(), False),
    StructField("InvoiceDate", DateType(), False),
    StructField("UnitPrice", FloatType(), False),
    StructField("CustomerID", StringType(), False),
    StructField("Country", StringType(), False)
])

df = spark.read.format("csv") \
        .opion("header", "True") \
        .option("sep", ",") \
        .load("../data/Onleine Retail.csv", schema=schema)

df.show()

df.dtypes

## 列を指定してデータの抽出
df.select("StockCode", "Description").show()
# 行数カウント
df.count()


## 条件を満たすレコード抽出
df.filter(df["UnitPrice"] > 30).show()
df.fileter(df["Country"]=='Switzerland').show()


## Whereを使用したレコードの抽出
# show(trancate=False)で隠れてる文字も表示
df.where(df["Description"].contains("WATER")).show(trancate=False)

## 列の作成と削除
# WithColumn
df = df.withColumn("amount", df["UnitPrice"]*df["Quantity"]).show()
df.show()
# DROP
df = df.drop("amount", "StockCode").show()
df.show()

## 列名の変更
#withColumnRenamed

##  キャスト(型変換)
df.dtypes
df.withColumn("Quantity", df["Quantity"].cast("float"))

## 日付の処理
df.show()
df.dtypes
# インポート
from pyspark.sql.functions import year, month, dayofmonth
# 年のカラム作成
df_tmp = df.withColumn("paurchased_year", year("InvoiceDate"))
df_tmp.show()
# 月のカラム作成
df_tmp = df.withColumn("paurchased_month", month("InvoiceDate"))
df_tmp.show()
# 日付のカラム作成
df_tmp = df.withColumn("paurchased_day", dayofmonth("InvoiceDate"))
df_tmp.show()

df.withColumnRenamed("Country", "Country_name").show()
# 以下でカラム名だけ表示も可能
df.withColumnRenamed("Country", "Country_name").columns

## 欠損値処理
df.show()
#isNull
df[df["InvoiceNo"].isNUll()].show()
# nanチェック isnan
#インポートが必要
from pyspark.sql.functions import isnan
df[isnan(df["Description"])].count()
#欠損値の穴埋め方法 fillna
df.fillna("unknown", subset=["Description"])
df.filter(df["DEscription"]=="unknown").count()
#欠損値の削除 drapna
df[df["CustomerID"].isNull()].count()
df.dropna("any").count()

