## PySparkの準備
from pyspark.sql import SparkSession

## SparkSessionを作る
spark = SparkSession.builder.appName('Data_wrangling').getOrCreate()

## ファイルの読み込み(",区切りでheaderはそのまま使う")
# .option("inferSchema", "True") で自動でスキーマ作成
df = spark.read.format("csv") \
        .option("inferSchema", "True") \
        .opion("header", "True") \
        .option("sep", ",") \
        .load("../data/Onleine Retail.csv")
df
df.show() #実際のデータを表示

#各カラムの型・nullable = trueがわかる
# TRUE　でnull許容
df.printSchema() 

type(df) 


## スキーマ指定の型を自分で決めて取り込む
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType

schema = StructType([
    StructField("InvoiceNo", StringType(), False),
    StructField("StockCode", StringType(), False),
    StructField("Description", StringType(), False),
    StructField("Quantity", IntegerType(), False),
    StructField("InvoiceDate", DateType(), False),
    StructField("UnitPrice", FloatType(), False),
    StructField("CustomerID", StringType(), False),
    StructField("Country", StringType(), False)
])

df = spark.read.format("csv") \
        .opion("header", "True") \
        .option("sep", ",") \
        .load("../data/Onleine Retail.csv", schema=schema)

df.show()

df.dtypes

## 列を指定してデータの抽出
df.select("StockCode", "Description").show()
# 行数カウント
df.count()


## 条件を満たすレコード抽出
df.filter(df["UnitPrice"] > 30).show()
df.fileter(df["Country"]=='Switzerland').show()


## Whereを使用したレコードの抽出
# show(trancate=False)で隠れてる文字も表示
df.where(df["Description"].contains("WATER")).show(trancate=False)

## 列の作成と削除
# WithColumn
df = df.withColumn("amount", df["UnitPrice"]*df["Quantity"]).show()
df.show()
# DROP
df = df.drop("amount", "StockCode").show()
df.show()

## 列名の変更
#withColumnRenamed
df.withColumnRenamed("Country", "Country_name").show()
# 以下でカラム名だけ表示も可能
df.withColumnRenamed("Country", "Country_name").columns


