## PySparkの準備
from pyspark.sql import SparkSession

## SparkSessionを作る
spark = SparkSession.builder.appName('Data_wrangling').getOrCreate()

## ファイルの読み込み(",区切りでheaderはそのまま使う")
# .option("inferSchema", "True") で自動でスキーマ作成
df = spark.read.format("csv") \
        .option("inferSchema", "True") \
        .opion("header", "True") \
        .option("sep", ",") \
        .load("../data/Onleine Retail.csv")
df
df.show() #実際のデータを表示

#各カラムの型・nullable = trueがわかる
# TRUE　でnull許容
df.printSchema() 

type(df) 


## スキーマ指定の型を自分で決めて取り込む
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType

schema = StructType([
    StructField("InvoiceNo", StringType(), False),
    StructField("StockCode", StringType(), False),
    StructField("Description", StringType(), False),
    StructField("Quantity", IntegerType(), False),
    StructField("InvoiceDate", DateType(), False),
    StructField("UnitPrice", FloatType(), False),
    StructField("CustomerID", StringType(), False),
    StructField("Country", StringType(), False)
])

df = spark.read.format("csv") \
        .opion("header", "True") \
        .option("sep", ",") \
        .load("../data/Onleine Retail.csv", schema=schema)

df.show()

df.dtypes

## 列を指定してデータの抽出
df.select("StockCode", "Description").show()
# 行数カウント
df.count()


## 条件を満たすレコード抽出
df.filter(df["UnitPrice"] > 30).show()
df.fileter(df["Country"]=='Switzerland').show()


## Whereを使用したレコードの抽出
# show(trancate=False)で隠れてる文字も表示
df.where(df["Description"].contains("WATER")).show(trancate=False)

## 列の作成と削除
# WithColumn
df = df.withColumn("amount", df["UnitPrice"]*df["Quantity"]).show()
df.show()
# DROP
df = df.drop("amount", "StockCode").show()
df.show()

## 列名の変更
#withColumnRenamed

##  キャスト(型変換)
df.dtypes
df.withColumn("Quantity", df["Quantity"].cast("float"))

## 日付の処理
df.show()
df.dtypes
# インポート
from pyspark.sql.functions import year, month, dayofmonth
# 年のカラム作成
df_tmp = df.withColumn("paurchased_year", year("InvoiceDate"))
df_tmp.show()
# 月のカラム作成
df_tmp = df.withColumn("paurchased_month", month("InvoiceDate"))
df_tmp.show()
# 日付のカラム作成
df_tmp = df.withColumn("paurchased_day", dayofmonth("InvoiceDate"))
df_tmp.show()

df.withColumnRenamed("Country", "Country_name").show()
# 以下でカラム名だけ表示も可能
df.withColumnRenamed("Country", "Country_name").columns

## 欠損値処理
df.show()
#isNull
df[df["InvoiceNo"].isNUll()].show()
# nanチェック isnan
#インポートが必要
from pyspark.sql.functions import isnan
df[isnan(df["Description"])].count()
#欠損値の穴埋め方法 fillna
df.fillna("unknown", subset=["Description"])
df.filter(df["DEscription"]=="unknown").count()
#欠損値の削除 drapna
df[df["CustomerID"].isNull()].count()
df.dropna("any").count()


## 集計groupby
df.show()

##Description列ごとにカウントする
df.groupby(df["Description"]).count().sho()
# 降順処理も加える
# インポート
from pyspark.sql.functions import desc, asc
df.groupby(df["Description"]).count().sort(desc("count")).show(trancate=False)
# 購入した商品の中で1000回以上購入されてる商品を小さい順に並べ替え
df.groupby(df["Description"]).count().filter("`count` >= 1000").sort(asc("count")).show(trancate=False)


## 統計量
df.show()
df.describe(["Quantity", "UnitPrice"]).show()
# describeと同じ
df.select(["Quantity", "UnitPrice"]).summary().show()
# 平均(Groupbyで集計する)
df.groupBy("Country").mean("Quantity").show()
# 平均(aggで集計する)
df.agg({"Quantity": "mean"}).show()

## 条件分岐と定数
# 定数作成用のlitをインポート
from pyspark.sql.functions import lit
df.show()
df.withColumn('test',lit(1)).show()

#when,colをインポート
from pyspark.sql.functions import when, col
df_tmp = df.withColumn('cancel_value', when(col("Quantity") < 0, lit("9999")).otherwise(lit("1")))
df.show()
#実際に上記条件で分けられてるfilterで確認
df_tmp.filter(df_tmp["cancel_value"]==9999).show()


## ユニーク
df.select("Country").distinct().show()

## 重複行の削除
df.dropDuplicates(["InvoiceNo"]).count()

#whenによる条件分岐
from pyspark.sql.functions import when
df_new = df.withColumn("PriceCategory",
                    when(df["UnitPrice"]>15, "expensive")
                        .when((df["UnitPrice"]<=15) & (df["UnitPrice"]>5), "medium")
                        .when((df["UnitPrice"]<=5) & (df["UnitPrice"]>0), "cheap")
                        .otherwise("unknown")
                       )

## Pandasのフレームワークへ変換
#pandasへ
df_pandas = df.toPanadas()
df_pandas
#sparkへ
df_pyspark = spark.createDataFrame(df_pandas)
df_pyspark.show()


## DataFrame作成とJoin
df1 = spark.createDataFrame([(1, "apple"),(2, "orange"), (3, None), (4, "banana"), (None, "grapes"),["id", "value1"]])
df1.show()
df2 = spark.createDataFrame([(2, "orange"),(4, "banana"), (6, "None"),(8, "carrot"),["id", "value2"]])
df1.show()
#inner join
df1.join(df2, "id", "inner").show()
#left join
df1.join(df2, "id", "leftouter").show()


## ファイル出力
df.show()
df.write.format("csv").option("delimiter",",").save("save_dataframe")

#一つのcsvにする
df.coalesce(1).write.format("csv").option("delimiter",",").save("./data/save_coalesce_coalacdataframe")

# パーティション分けをして保存
#yearとmonthで列を作って分ける
df_tmp = df.withColumn("purchase_year", year(InvoiceDate)) \
            .withColumn("purchase_month", month(InvoiceDate))
df_tmp.show()

df_tmp.write.partitionBy("purchase_year", "purchase_month").format("csv") \
        .option("delimiter", ",").save("./data/save_partiotion_dataframe")

