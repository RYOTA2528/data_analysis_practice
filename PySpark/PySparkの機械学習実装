### ステップ①：特徴量ベクトルの作成・それをもとに線形重回帰モデルを生成
# 1. データの要約表示（すでにインポート済みのdfを想定）
df.summary().show()

# 2. 線形回帰で使う変数の抽出
linear_df = df.select(["age", "balance", "campaign"])

# 3. 目的変数（ターゲット）の指定
target = "balance" 

# 4. 説明変数（特徴量）の指定
features = ["age", "campaign"]

# 5. モデルに使う特徴量データの作成
tran_df = df.select(features)

# 6. 特徴量ベクトルの作成（VectorAssembler）
from pyspark.ml.feature import VectorAssembler
#、PySparkのMLlibで機械学習モデルに入力するための「特徴量ベクトル列」を作成する準備
assemble = VectorAssembler(inputCols=features, outputCol="features") 

# (解説)特徴量ベクトルの生成
# VectorAssembler は、複数の列を1つのベクトル列にまとめる。
# MLlibのモデル（例：線形回帰）は features という1列のベクトルを入力とする。

# 線形重回帰モデリングステージ
from pyspark.ml.regression import LinearRegression
# 線形回帰モデルのインスタンスを作成(featureCol="features"は特徴量ベクトルを格納した列、labelCol="balance"は目的変数（預金残高）を表す列。)
clf = LinearRegression(featureCol="features", labelCol="balance")

#上記２つのステージを使ったパイプラインの設定：ステージの登録
from pyspark.ml.pipeline import Pipeline
pipeline = Pipeline(stages=[assemble, clf])
mode = pipeline.fit(linear_df) #目的変数も含まれるデータを渡す


# パイプラインの実行(各ステージを実行):VectorAssembleで作成した特徴量のベクトルをもとに実際の予測値を出す
df = model.transform(linear_df)
df.show()
################「predictionという線形重回帰による予測値が出来上がる」

### ステップ②：係数や接点の確認をする

##係数の確認：係数ベクトル。説明変数ごとの重みを表す(w1x1+w2x2 ...+b)のw1の1のこと
model.stages[1].coefficients　# .coefficients は、その回帰モデルが学習した係数
↓
（解説）
features = ["age", "campaign"]
とした場合に：
model.coefficients
# → 結果：DenseVector([12.3, -1.5])
これは次の意味です：
age の係数 = 12.3
→ age が 1 増えると、balance が 約 12.3 増える
campaign の係数 = -1.5
→ campaign が 1 増えると、balance が 約 1.5 減る


## 切片の確認(+b)：すべての説明変数（特徴量）が0のときの予測値
model.stages[1].intercept
↓
（解説）
たとえば：
intercept = 1000.0
ならば、age = 0 かつ campaign = 0 のとき、balance（予測値）は 1000 になるという意味


### ステップ③：学習データとテストデータを分け、データ作成ステージを用意、線形重回帰モデルに学習データをあてて実行
#学習データ7割、テストデータ3割の例
tran_df, test_df = data.select(["age","balance", "campaign"]) \
                      .randomSplit([0.7, 0.3], seed = 1)
↓
（解説）seed=1 を指定することで、結果のランダム性を固定します（再現性のある分割になる）

tran_df.show()

#データ作成ステージを用意(特徴量ベクトルの生成)
from pyspark.ml.feature import VectorAssembler
target = "balance"
features = ["age", "campaign"]
assmble = VectorAssembler(inputCols=features, outputCol="features") #特徴量ベクトル生成

# 線形重回帰モデルの作成(実際にtrain_dfをmodelに使用してみる)
（sklearn の評価指標（metrics）から 平均二乗誤差（MSE） を使うためのインポート。）
# sklearnの平均二乗誤差関数（MSE）をインポート
from sklearn.metrics import mean_squared_error

# NumPyの平方根関数を使うためにインポート（RMSEを計算するため）
import numpy as np

# PySparkのDataFrame（pred_train）をPandasのDataFrameに変換
# → scikit-learnはPandasまたはNumPy形式のデータを使うため
pred_train_pandas = pred_train.toPandas()

# RMSEの計算：
# ① mean_squared_error で MSE（平均二乗誤差）を求める
# ② np.sqrt で MSE の平方根を取り、RMSE（平方根平均二乗誤差）に変換
rmse = np.sqrt(mean_squared_error(pred_train_pandas["balance"],  # 正解ラベル（実測値）
                                  pred_train_pandas["prediction"]))  # モデルの予測値

# 結果を出力
print("RMSE:", rmse)

（おまけ）pandasでの係数の出し方
train_cols = train_df.columns
train_cols.remove(target)
pd.DataFrame(index=train_cols, data=model.stages[1].coefficients, columns=["coefficients"])

### ステップ④：テストデータによる予測
test_df.show()
test_df.count()
pred_test = model.transform(test_df)
pred_test.show()

# sklearnの平均二乗誤差関数（MSE）をインポート
from sklearn.metrics import mean_squared_error

# NumPyの平方根関数を使うためにインポート（RMSEを計算するため）
import numpy as np

# PySparkのDataFrame（pred_train）をPandasのDataFrameに変換
# → scikit-learnはPandasまたはNumPy形式のデータを使うため
pred_test_pandas = pred_test.toPandas()

# RMSEの計算：
# ① mean_squared_error で MSE（平均二乗誤差）を求める
# ② np.sqrt で MSE の平方根を取り、RMSE（平方根平均二乗誤差）に変換
rmse = np.sqrt(mean_squared_error(pred_test_pandas["balance"],  # 正解ラベル（実測値）
                                  pred_test_pandas["prediction"]))  # モデルの予測値
