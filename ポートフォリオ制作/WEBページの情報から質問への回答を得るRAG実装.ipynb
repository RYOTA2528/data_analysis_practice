{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup #ウェブスクレイピングでHTMLデータを解析し、データ抽出\n",
        "from openai import OpenAI\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPEN_API_KEY\"] = userdata.get('OPEN_API_KEY')\n",
        "# OpenAIクライアントを初期化\n",
        "client = OpenAI(api_key= os.environ[\"OPEN_API_KEY\"])\n",
        "\n",
        "# 先ほど作成したスクレイピング処理の関数化\n",
        "def scrape_article(url):\n",
        "# 対象のURLを取得（スクレイピング処理）\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.text, \"html.parser\") #対象urlのhtmlを取得\n",
        "  print(soup)\n",
        "\n",
        "  # テキストの情報だけを取得してくる\n",
        "  text_nodes = soup.find_all(\"div\")\n",
        "  len(text_nodes) #divで囲まれてる箇所何か所あるか確認\n",
        "  for t in text_nodes:\n",
        "    print(t.text) #t.textでtext属性のものだけを取得できる\n",
        "\n",
        "  # 上記テキスト情報をリストへ格納\n",
        "  t_all = []\n",
        "  for t in text_nodes:\n",
        "    t_all.append(t.text.replace(\"\\n\",\"\").replace(\"\\t\",\"\")) #\\n,\\tを\"\"に変換\"\n",
        "    print(t_all)\n",
        "\n",
        "  #文字列をまずはすべて結合させる\n",
        "  joined_text =  \"\".join(t_all)\n",
        "  return joined_text\n",
        "\n",
        "\n",
        "# chunkの取得処理も関数化\n",
        "# 400文字で一区切り、50文字戻り350+50で区切っていく\n",
        "def chunk_text(text,chunk_size,overlap,):\n",
        "  chunks = []\n",
        "  start = 0\n",
        "\n",
        "  while start + chunk_size <= len(text): #この文字数までは回し続ける\n",
        "    chunks.append(text[start:start + chunk_size]) #最初は0:400文字まではいってる\n",
        "    start += (chunk_size - overlap)\n",
        "  #上記処理では最後が例えばstart:1100だと残り分を残したまま処理が終わってしまうため残り分を算出\n",
        "  if start < len(text):\n",
        "    chunks.append(text[-chunk_size:])\n",
        "\n",
        "  return chunks\n",
        "\n",
        "# テキストをベクトル化する関数処理流用\n",
        "  #client.embeddingsでembeddingsAPIをcallする\n",
        "def vectorize_text(text):\n",
        "  response = client.embeddings.create(\n",
        "      input = text,\n",
        "      model = \"text-embedding-3-small\"\n",
        "  )\n",
        "  return response.data[0].embedding\n",
        "\n",
        "# documentsの中で最も高い類似度を格納する処理も関数化\n",
        "def find_most_similar(question_vector, vectors, documents):\n",
        "  max_similarity = 0\n",
        "  most_similar_index = 0\n",
        "\n",
        "  for index, vector in enumerate(vectors):\n",
        "    similarity = cosine_similarity([question_vector], [vector])[0][0]\n",
        "    print(documents[index], \":\", similarity) #処理の流れ確認\n",
        "    if similarity > max_similarity:\n",
        "      max_similarity = similarity\n",
        "      most_similar_index = index\n",
        "\n",
        "  return documents[most_similar_index]\n",
        "\n",
        "# 質問への回答を出力するプロンプトも関数化\n",
        "def ask_question(question_context, context):\n",
        "  prompt = f''' 以下の質問に以下情報から答えてください。\n",
        "  [ユーザへの質問]\n",
        "  {question}\n",
        "\n",
        "  [情報]\n",
        "  {documents[most_similar_index]}\n",
        "  '''\n",
        "  print(prompt)\n",
        "  response = client.completions.create(\n",
        "      model = \"gpt-3.5-turbo-instruct\",\n",
        "      prompt=prompt,\n",
        "      max_tokens = 200\n",
        "  )\n",
        "\n",
        "  return response.choices[0].text\n"
      ],
      "metadata": {
        "id": "N1w63ttNtbOM",
        "outputId": "04afac14-41e9-4e9f-c056-67407745a183",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "id": "N1w63ttNtbOM",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "OpenAI.__init__() takes 1 positional argument but 2 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ecb4631cbb99>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"OPEN_API_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'OPEN_API_KEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# OpenAIクライアントを初期化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"OPEN_API_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# 先ほど作成したスクレイピング処理の関数化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: OpenAI.__init__() takes 1 positional argument but 2 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 実際にWEBページの情報をもとに回答プロンプトを実装"
      ],
      "metadata": {
        "id": "OmmyaW-7tdFz"
      },
      "id": "OmmyaW-7tdFz"
    },
    {
      "cell_type": "code",
      "source": [
        "# 引数\n",
        "url = \"https://toukei-lab.com/achademy/?page_id=1619\"\n",
        "chunk_size = 400\n",
        "overlap = 50\n",
        "\n",
        "\n",
        "# 実際に作成した関数にて挙動の確認\n",
        "article_text = scrape_article(url)\n",
        "text_chunks = chunk_text(article_text, chunk_size, overlap) #chunksの中にurlの文字列を格納\n",
        "\n",
        "# documents(text_chunks)とquestionに分けていく作業\n",
        "# chunksのchunkを一つ一つ取り出し、ベクトル化処理に入れ込む\n",
        "vectors = [vectorize_text(doc) for doc in text_chunks]\n",
        "\n",
        "question = \"オーダーメイドプランの価格はいくら？\"\n",
        "question_vector = vectorize_text(question)\n",
        "\n",
        "# documentsの中から最も高い類似度のもの(答え)を格納\n",
        "similar_document = find_most_similar(question_vector, vectors,text_chunks)\n",
        "\n",
        "# 質問への回答を出力するプロンプトにて\n",
        "answer = ask_question(question, similar_document)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "C-ESvdyGtiMR"
      },
      "id": "C-ESvdyGtiMR",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}