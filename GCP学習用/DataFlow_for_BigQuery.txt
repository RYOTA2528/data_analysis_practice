/*
【Dataflowの特徴】

*/


/*
【ハンズオンの内容】
ューヨーク市のタクシー車両を多数所有しているものと仮定して、リアルタイムでビジネスの状況をモニタリングします。タクシーの収益、乗客数、乗車状況などを把握し、その結果を管理ダッシュボードで視覚化するためのストリーミング データ パイプラインを構築します。
*/


/*
【ハンズオンの流れ】
・テンプレートからの Dataflow ジョブの作成
・BigQuery への Dataflow パイプラインのストリーミング
・BigQuery での Dataflow パイプラインのモニタリング
・SQL を使用した結果の分析
・Looker Studio での主要指標の可視化
*/


/*
【作業内容】
・gcloud で以下command実施
　・gcloud auth list　：有効なアカウント名を一覧表示
　・gcloud config list project　：プロジェクト ID を一覧表示

(タスク 1). BigQuery データセットを作成する
→taxirides データセットを作成します。Google Cloud Shell か Google Cloud コンソールを使用してこのデータセットを作成
BigQuery 内のテーブルは、データセットに編成されます。このラボでは、タクシーに関するデータがスタンドアロン ファイルから Dataflow 経由で流れ、BigQuery に保存されます。この設定では、ソースの Cloud Storage バケットに付与された新しいデータファイルはすべて、読み込み用に自動処理されます。

（方法 1: コマンドライン ツール）
　①Cloud Shell（Cloud Shell アイコン）で、次のコマンドを実行して taxirides データセットを作成します。
　　・bq --location=Region mk taxirides
　②以下のコマンドを実行して taxirides.realtime テーブルを作成します（この空のスキーマに後でデータをストリーミングします　　・bq --location=Region mk \
--time_partitioning_field timestamp \
--schema ride_id:string,point_idx:integer,latitude:float,longitude:float,\
timestamp:timestamp,meter_reading:float,meter_increment:float,ride_status:string,\
passenger_count:integer -t taxirides.realtime

（方法 2: BigQuery コンソール UI）
　①Google Cloud コンソールのナビゲーション メニュー（ナビゲーション メニュー）で、[BigQuery] をクリックします。
　②ようこそのダイアログが表示されたら、[完了] をクリックします。
　③プロジェクト ID の横に表示される [アクションを表示] （アクションを表示）をクリックし、[データセットを作成] をクリックします。
　④[データセット ID] に「taxirides」と入力します。
　⑤データのロケーション で以下を選択します。
Region
　⑥次に、[データセットを作成] をクリックします。
　⑦[エクスプローラ] ペインで [ノードを展開します]（展開）をクリックして新しい taxirides データセットを表示します。
　⑧taxirides データセットの横の アクションを表示（アクションを表示）をクリックして、次に [開く] をクリックします。
　⑨[テーブルを作成] をクリックします。
 10[テーブル] に「realtime」と入力します。
 11.[スキーマ] で [テキストとして編集] をクリックし、以下のスキーマを貼り付けます。
ride_id:string,
point_idx:integer,
latitude:float,
longitude:float,
timestamp:timestamp,
meter_reading:float,
meter_increment:float,
ride_status:string,
passenger_count:integer
 12[パーティションとクラスタの設定] で [タイムスタンプ] を選択します。
 13 [テーブルを作成] をクリックします。


(タスク 2. ラボに必要なアーティファクトをコピーする)
　①Cloud Shell（Cloud Shell アイコン）で、次のコマンドを実行して Dataflow ジョブに必要なファイルを移動します。
gcloud storage cp gs://cloud-training/bdml/taxisrcdata/schema.json  gs://Project_ID-bucket/tmp/schema.json
gcloud storage cp gs://cloud-training/bdml/taxisrcdata/transform.js  gs://Project_ID-bucket/tmp/transform.js
gcloud storage cp gs://cloud-training/bdml/taxisrcdata/rt_taxidata.csv  gs://Project_ID-bucket/tmp/rt_taxidata.csv
　②
　
（タスク 3）. Dataflow パイプラインを設定する
※Cloud Storage バケットからファイルを読み取り、BigQuery にデータを書き込むストリーミング データ パイプラインを設定
　①Dataflow API への接続を再起動する
　　・Cloud Shell で次のコマンドを実行して、Dataflow API がプロジェクト内でスムーズに有効化されるようにします。
　　　gcloud services disable dataflow.googleapis.com
　　　gcloud services enable dataflow.googleapis.com
　②新しいストリーミング パイプラインを作成する
・ [Dataflow] をクリック
・テンプレートからジョブを作成] をクリック
・ジョブ名として「streaming-taxi-pipeline」と入力
・リージョン エンドポイントでRegionを選択
・[Dataflow テンプレート] で、[Process Data Continuously (stream)] にある [Cloud Storage Text to BigQuery (Stream)] テンプレートを選択
・[The GCS Input File you'd like to process] に、以下のコマンドを貼り付け
　Project_ID-bucket/tmp/rt_taxidata.csv
・[GCS location of your BigQuery schema file, described as a JSON] に、以下のコマンドを貼り付ける
　Project_ID-bucket/tmp/schema.json
・[Output table to write to] に、以下のコマンドを貼り付ける
　Project_ID:taxirides.realtime
・[BigQuery 読み込みプロセスで使用する一時ディレクトリ] に、以下のコマンドを貼り付ける
　Project_ID-bucket/tmp
・[必須パラメータ] をクリック
・一時ファイルの書き込みに使用する [一時的な場所] に、以下のコマンドを貼り付ける
　Project_ID-bucket/tmp
・[Cloud Storage 内の JavaScript UDF パス] に、以下のコマンドを貼り付ける
　Project_ID-bucket/tmp/transform.js
・[UDF JavaScript Name] に、以下のコマンドを貼り付ける
　transform
・[最大ワーカー数] に「2」と入力します。
[ワーカーの数] に「1」と入力します。
[デフォルトのマシンタイプを使用する] チェックボックスをオフにします。
[汎用] で次の設定を選択します。
シリーズ: E2
マシンタイプ: e2-medium（2 個の vCPU、4 GB メモリ）
・ジョブを実行

（タスク 4）. BigQuery を使用してタクシーデータを分析する
①[BigQuery] をクリックし以下クエリの実行
SELECT * FROM taxirides.realtime LIMIT 10

（タスク 5）. レポート用にストリームの集計を実行する
①以下のサブクエリ及び集計用のクエリを実行
WITH streaming_data AS (

SELECT
  timestamp,
  TIMESTAMP_TRUNC(timestamp, HOUR, 'UTC') AS hour,
  TIMESTAMP_TRUNC(timestamp, MINUTE, 'UTC') AS minute,
  TIMESTAMP_TRUNC(timestamp, SECOND, 'UTC') AS second,
  ride_id,
  latitude,
  longitude,
  meter_reading,
  ride_status,
  passenger_count
FROM
  taxirides.realtime
ORDER BY timestamp DESC
LIMIT 1000

）

# レポート用にストリームの集計を行います。
SELECT
 ROW_NUMBER() OVER() AS dashboard_sort,
 minute,
 COUNT(DISTINCT ride_id) AS total_rides,
 SUM(meter_reading) AS total_revenue,
 SUM(passenger_count) AS total_passengers
FROM streaming_data
GROUP BY minute, timestamp


②[保存] > [クエリを保存] をクリック

（タスク 6）. Dataflow ジョブを停止する
※Dataflow ジョブを停止して、プロジェクト用のリソースを開放します。
① [Dataflow] をクリック
②[streaming-taxi-pipeline] または新しいジョブ名をクリックし[キャンセル] > [ジョブの停止] を選択

（タスク 7）. リアルタイム ダッシュボードを作成する
リアルタイム ダッシュボードを作成して、データを可視化
①My Saved Query] をクリックしその後実行
②BigQuery で、[データを探索] > [Looker Studio で調べる] をクリックしGet started] をクリック
③Looker Studio ウィンドウで棒グラフをクリック
④[グラフを追加] をクリックし、[複合グラフ] を選択
⑤[設定] ペインの [期間のディメンション] で、[minute (Date)] にカーソルを合わせて [X] をクリックして削除
⑥[データ] ペインで、[dashboard_sort] をクリックして、[設定] > [期間のディメンション] > [ディメンションを追加] にドラッグ
⑦[設定] > [ディメンション] で、[分] をクリックし、[dashboard_sort] を選択
⑧[設定] > [指標] で、[dashboard_sort] をクリックし、[total_rides] を選択
⑨[設定] > [指標] で、[Record Count] をクリックし、[total_passengers] を選択
10[設定] > [指標] で、[指標を追加] をクリックし、[total_revenue] を選択
11[設定] > [並べ替え] で、[total_rides] をクリックし、[dashboard_sort] を選択
12[設定] > [並べ替え] で、[昇順] をクリック

　*/
