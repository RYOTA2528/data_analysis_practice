/*
【Dataflowの特徴】

*/


/*
【ハンズオンの内容】
ューヨーク市のタクシー車両を多数所有しているものと仮定して、リアルタイムでビジネスの状況をモニタリングします。タクシーの収益、乗客数、乗車状況などを把握し、その結果を管理ダッシュボードで視覚化するためのストリーミング データ パイプラインを構築します。
*/


/*
【ハンズオンの流れ】
・テンプレートからの Dataflow ジョブの作成
・BigQuery への Dataflow パイプラインのストリーミング
・BigQuery での Dataflow パイプラインのモニタリング
・SQL を使用した結果の分析
・Looker Studio での主要指標の可視化
*/


/*
【作業内容】
・gcloud で以下command実施
　・gcloud auth list　：有効なアカウント名を一覧表示
　・gcloud config list project　：プロジェクト ID を一覧表示

(タスク 1). BigQuery データセットを作成する
→taxirides データセットを作成します。Google Cloud Shell か Google Cloud コンソールを使用してこのデータセットを作成
BigQuery 内のテーブルは、データセットに編成されます。このラボでは、タクシーに関するデータがスタンドアロン ファイルから Dataflow 経由で流れ、BigQuery に保存されます。この設定では、ソースの Cloud Storage バケットに付与された新しいデータファイルはすべて、読み込み用に自動処理されます。

（方法 1: コマンドライン ツール）
　①Cloud Shell（Cloud Shell アイコン）で、次のコマンドを実行して taxirides データセットを作成します。
　　・bq --location=Region mk taxirides
　②以下のコマンドを実行して taxirides.realtime テーブルを作成します（この空のスキーマに後でデータをストリーミングします　　・bq --location=Region mk \
--time_partitioning_field timestamp \
--schema ride_id:string,point_idx:integer,latitude:float,longitude:float,\
timestamp:timestamp,meter_reading:float,meter_increment:float,ride_status:string,\
passenger_count:integer -t taxirides.realtime

（方法 2: BigQuery コンソール UI）
　①Google Cloud コンソールのナビゲーション メニュー（ナビゲーション メニュー）で、[BigQuery] をクリックします。
　②ようこそのダイアログが表示されたら、[完了] をクリックします。
　③プロジェクト ID の横に表示される [アクションを表示] （アクションを表示）をクリックし、[データセットを作成] をクリックします。
　④[データセット ID] に「taxirides」と入力します。
　⑤データのロケーション で以下を選択します。
Region
　⑥次に、[データセットを作成] をクリックします。
　⑦[エクスプローラ] ペインで [ノードを展開します]（展開）をクリックして新しい taxirides データセットを表示します。
　⑧taxirides データセットの横の アクションを表示（アクションを表示）をクリックして、次に [開く] をクリックします。
　⑨[テーブルを作成] をクリックします。
 10[テーブル] に「realtime」と入力します。
 11.[スキーマ] で [テキストとして編集] をクリックし、以下のスキーマを貼り付けます。
ride_id:string,
point_idx:integer,
latitude:float,
longitude:float,
timestamp:timestamp,
meter_reading:float,
meter_increment:float,
ride_status:string,
passenger_count:integer
 12[パーティションとクラスタの設定] で [タイムスタンプ] を選択します。
 13 [テーブルを作成] をクリックします。

(
ラボの手順とタスク
概要
目標
設定と要件
タスク 1. BigQuery データセットを作成する
タスク 2. ラボに必要なアーティファクトをコピーする
タスク 3. Dataflow パイプラインを設定する
タスク 4. BigQuery を使用してタクシーデータを分析する
タスク 5. レポート用にストリームの集計を実行する
タスク 6. Dataflow ジョブを停止する
タスク 7. リアルタイム ダッシュボードを作成する
タスク 8. 時系列ダッシュボードを作成する
お疲れさまでした
ラボを終了する
Dataflow を使用してリアルタイム ダッシュボード用のストリーミング データ パイプラインを作成する
experiment
ラボ
schedule
1時間
universal_currency_alt
クレジット: 5
show_chart
入門
info
このラボでは、学習をサポートする AI ツールが組み込まれている場合があります。
概要
このラボでは、ニューヨーク市のタクシー車両を多数所有しているものと仮定して、リアルタイムでビジネスの状況をモニタリングします。タクシーの収益、乗客数、乗車状況などを把握し、その結果を管理ダッシュボードで視覚化するためのストリーミング データ パイプラインを構築します。

目標
このラボでは、次の方法について学びます。

テンプレートからの Dataflow ジョブの作成
BigQuery への Dataflow パイプラインのストリーミング
BigQuery での Dataflow パイプラインのモニタリング
SQL を使用した結果の分析
Looker Studio での主要指標の可視化
設定と要件
[ラボを開始] ボタンをクリックする前に
こちらの手順をお読みください。ラボの時間は制限されており、一時停止することはできません。[ラボを開始] ボタンをクリックするとタイマーが開始され、Cloud リソースを利用できる時間が表示されます。

この Qwiklabs ハンズオンラボでは、シミュレーションやデモ環境ではなく、実際のクラウド環境を使ってご自身でラボのアクティビティを行うことができます。一時的な認証情報が新しく提供されるため、ラボ受講中の Google Cloud Platform へのログインおよびアクセスにはその認証情報を使用してください。

前提条件
このラボを完了するには、次のものが必要です。

標準的なインターネット ブラウザ（Chrome を推奨）。
ラボを完了するために必要な時間。
注: すでに個人の GCP アカウントやプロジェクトをお持ちの場合でも、そのアカウントやプロジェクトはラボでは使用しないでください。

ラボを開始してコンソールにログインする方法
[ラボを開始] ボタンをクリックします。ラボの料金をお支払いいただく必要がある場合は、表示されるポップアップでお支払い方法を選択してください。 左側のパネルには、このラボで使用する必要がある一時的な認証情報が表示されます。

[認証情報] パネル

ユーザー名をコピーし、[Google Console を開く] をクリックします。 ラボでリソースが起動し、別のタブで [アカウントの選択] ページが表示されます。

注: タブをそれぞれ別のウィンドウで開き、並べて表示しておきましょう。
[アカウントの選択] ページで [別のアカウントを使用] をクリックします。[ログイン] ページが開きます。

[別のアカウントを使用] オプションがハイライト表示されている、アカウントのダイアログ ボックスを選択します。

[接続の詳細] パネルでコピーしたユーザー名を貼り付けます。パスワードもコピーして貼り付けます。

注: 認証情報は [接続の詳細] パネルに表示されたものを使用してください。Google Cloud Skills Boost の認証情報は使用しないでください。請求が発生する事態を避けるため、Google Cloud アカウントをお持ちの場合でも、このラボでは使用しないでください。
その後次のように進みます。
利用規約に同意してください。
一時的なアカウントなので、復元オプションや 2 要素認証プロセスは設定しないでください。
無料トライアルには登録しないでください。
しばらくすると、このタブで Cloud コンソールが開きます。

注: 左上にある [ナビゲーション メニュー] をクリックすると、Google Cloud のプロダクトやサービスのリストが含まれるメニューが表示されます。 Cloud コンソール メニュー
Google Cloud Shell の有効化
Google Cloud Shell は、開発ツールと一緒に読み込まれる仮想マシンです。5 GB の永続ホーム ディレクトリが用意されており、Google Cloud で稼働します。

Google Cloud Shell を使用すると、コマンドラインで Google Cloud リソースにアクセスできます。

Google Cloud コンソールで、右上のツールバーにある [Cloud Shell をアクティブにする] ボタンをクリックします。

ハイライト表示された Cloud Shell アイコン

[続行] をクリックします。

環境がプロビジョニングされ、接続されるまでしばらく待ちます。接続した時点で認証が完了しており、プロジェクトに各自のプロジェクト ID が設定されます。次に例を示します。

Cloud Shell ターミナルでハイライト表示されたプロジェクト ID

gcloud は Google Cloud のコマンドライン ツールです。このツールは、Cloud Shell にプリインストールされており、タブ補完がサポートされています。

次のコマンドを使用すると、有効なアカウント名を一覧表示できます。
gcloud auth list
コピーしました
出力:

Credentialed accounts:
 - <myaccount>@<mydomain>.com (active)
</mydomain></myaccount>
出力例:

Credentialed accounts:
 - google1623327_student@qwiklabs.net
次のコマンドを使用すると、プロジェクト ID を一覧表示できます。
gcloud config list project
コピーしました
出力:

[core]
project = <project_id>
</project_id>
出力例:

[core]
project = qwiklabs-gcp-44776a13dea667a6
注: gcloud ドキュメントの全文については、 gcloud CLI の概要ガイド をご覧ください。
タスク 1. BigQuery データセットを作成する
このタスクでは、taxirides データセットを作成します。Google Cloud Shell か Google Cloud コンソールを使用してこのデータセットを作成できます。

このラボでは、NYC Taxi & Limousine Commission の公開データセットの抜粋を使用します。小さなカンマ区切りのデータファイルは、タクシーに関するデータの定期的な更新をシミュレートするために使用されます。

BigQuery はサーバーレス データ ウェアハウスです。BigQuery 内のテーブルは、データセットに編成されます。このラボでは、タクシーに関するデータがスタンドアロン ファイルから Dataflow 経由で流れ、BigQuery に保存されます。この設定では、ソースの Cloud Storage バケットに付与された新しいデータファイルはすべて、読み込み用に自動処理されます。

次のいずれかの方法で新しい BigQuery データセットを作成します。

方法 1: コマンドライン ツール
Cloud Shell（Cloud Shell アイコン）で、次のコマンドを実行して taxirides データセットを作成します。
bq --location=Region mk taxirides
コピーしました
以下のコマンドを実行して taxirides.realtime テーブルを作成します（この空のスキーマに後でデータをストリーミングします）。
bq --location=Region mk \
--time_partitioning_field timestamp \
--schema ride_id:string,point_idx:integer,latitude:float,longitude:float,\
timestamp:timestamp,meter_reading:float,meter_increment:float,ride_status:string,\
passenger_count:integer -t taxirides.realtime
コピーしました
方法 2: BigQuery コンソール UI
注: コマンドラインを使用してテーブルを作成した場合、この手順はスキップしてください。
Google Cloud コンソールのナビゲーション メニュー（ナビゲーション メニュー）で、[BigQuery] をクリックします。

ようこそのダイアログが表示されたら、[完了] をクリックします。

プロジェクト ID の横に表示される [アクションを表示] （アクションを表示）をクリックし、[データセットを作成] をクリックします。

[データセット ID] に「taxirides」と入力します。

データのロケーション で以下を選択します。

Region
コピーしました
次に、[データセットを作成] をクリックします。

[エクスプローラ] ペインで [ノードを展開します]（展開）をクリックして新しい taxirides データセットを表示します。

taxirides データセットの横の アクションを表示（アクションを表示）をクリックして、次に [開く] をクリックします。

[テーブルを作成] をクリックします。

[テーブル] に「realtime」と入力します。

[スキーマ] で [テキストとして編集] をクリックし、以下のスキーマを貼り付けます。

ride_id:string,
point_idx:integer,
latitude:float,
longitude:float,
timestamp:timestamp,
meter_reading:float,
meter_increment:float,
ride_status:string,
passenger_count:integer
コピーしました
[パーティションとクラスタの設定] で [タイムスタンプ] を選択します。

[テーブルを作成] をクリックします。

(タスク 2. ラボに必要なアーティファクトをコピーする)
　①Cloud Shell（Cloud Shell アイコン）で、次のコマンドを実行して Dataflow ジョブに必要なファイルを移動します。
gcloud storage cp gs://cloud-training/bdml/taxisrcdata/schema.json  gs://Project_ID-bucket/tmp/schema.json
gcloud storage cp gs://cloud-training/bdml/taxisrcdata/transform.js  gs://Project_ID-bucket/tmp/transform.js
gcloud storage cp gs://cloud-training/bdml/taxisrcdata/rt_taxidata.csv  gs://Project_ID-bucket/tmp/rt_taxidata.csv
　②
　
（タスク 3）. Dataflow パイプラインを設定する
※Cloud Storage バケットからファイルを読み取り、BigQuery にデータを書き込むストリーミング データ パイプラインを設定
　①Dataflow API への接続を再起動する
　　・Cloud Shell で次のコマンドを実行して、Dataflow API がプロジェクト内でスムーズに有効化されるようにします。
　　　gcloud services disable dataflow.googleapis.com
　　　gcloud services enable dataflow.googleapis.com
　②新しいストリーミング パイプラインを作成する
・ [Dataflow] をクリック
・テンプレートからジョブを作成] をクリック
・ジョブ名として「streaming-taxi-pipeline」と入力
・リージョン エンドポイントでRegionを選択
・[Dataflow テンプレート] で、[Process Data Continuously (stream)] にある [Cloud Storage Text to BigQuery (Stream)] テンプレートを選択
・[The GCS Input File you'd like to process] に、以下のコマンドを貼り付け
　Project_ID-bucket/tmp/rt_taxidata.csv
・[GCS location of your BigQuery schema file, described as a JSON] に、以下のコマンドを貼り付ける
　Project_ID-bucket/tmp/schema.json
・[Output table to write to] に、以下のコマンドを貼り付ける
　Project_ID:taxirides.realtime
・[BigQuery 読み込みプロセスで使用する一時ディレクトリ] に、以下のコマンドを貼り付ける
　Project_ID-bucket/tmp
・[必須パラメータ] をクリック
・一時ファイルの書き込みに使用する [一時的な場所] に、以下のコマンドを貼り付ける
　Project_ID-bucket/tmp
・[Cloud Storage 内の JavaScript UDF パス] に、以下のコマンドを貼り付ける
　Project_ID-bucket/tmp/transform.js
・[UDF JavaScript Name] に、以下のコマンドを貼り付ける
　transform
・[最大ワーカー数] に「2」と入力します。
[ワーカーの数] に「1」と入力します。
[デフォルトのマシンタイプを使用する] チェックボックスをオフにします。
[汎用] で次の設定を選択します。
シリーズ: E2
マシンタイプ: e2-medium（2 個の vCPU、4 GB メモリ）
・ジョブを実行

（タスク 4）. BigQuery を使用してタクシーデータを分析する
①[BigQuery] をクリックし以下クエリの実行
SELECT * FROM taxirides.realtime LIMIT 10

（タスク 5）. レポート用にストリームの集計を実行する
①以下のサブクエリ及び集計用のクエリを実行
WITH streaming_data AS (

SELECT
  timestamp,
  TIMESTAMP_TRUNC(timestamp, HOUR, 'UTC') AS hour,
  TIMESTAMP_TRUNC(timestamp, MINUTE, 'UTC') AS minute,
  TIMESTAMP_TRUNC(timestamp, SECOND, 'UTC') AS second,
  ride_id,
  latitude,
  longitude,
  meter_reading,
  ride_status,
  passenger_count
FROM
  taxirides.realtime
ORDER BY timestamp DESC
LIMIT 1000

）

# レポート用にストリームの集計を行います。
SELECT
 ROW_NUMBER() OVER() AS dashboard_sort,
 minute,
 COUNT(DISTINCT ride_id) AS total_rides,
 SUM(meter_reading) AS total_revenue,
 SUM(passenger_count) AS total_passengers
FROM streaming_data
GROUP BY minute, timestamp


②[保存] > [クエリを保存] をクリック

（タスク 6）. Dataflow ジョブを停止する
※Dataflow ジョブを停止して、プロジェクト用のリソースを開放します。
① [Dataflow] をクリック
②[streaming-taxi-pipeline] または新しいジョブ名をクリックし[キャンセル] > [ジョブの停止] を選択

（タスク 7）. リアルタイム ダッシュボードを作成する
リアルタイム ダッシュボードを作成して、データを可視化
①My Saved Query] をクリックしその後実行
②BigQuery で、[データを探索] > [Looker Studio で調べる] をクリックしGet started] をクリック
③Looker Studio ウィンドウで棒グラフをクリック
④[グラフを追加] をクリックし、[複合グラフ] を選択
⑤[設定] ペインの [期間のディメンション] で、[minute (Date)] にカーソルを合わせて [X] をクリックして削除
⑥[データ] ペインで、[dashboard_sort] をクリックして、[設定] > [期間のディメンション] > [ディメンションを追加] にドラッグ
⑦[設定] > [ディメンション] で、[分] をクリックし、[dashboard_sort] を選択
⑧[設定] > [指標] で、[dashboard_sort] をクリックし、[total_rides] を選択
⑨[設定] > [指標] で、[Record Count] をクリックし、[total_passengers] を選択
10[設定] > [指標] で、[指標を追加] をクリックし、[total_revenue] を選択
11[設定] > [並べ替え] で、[total_rides] をクリックし、[dashboard_sort] を選択
12[設定] > [並べ替え] で、[昇順] をクリック

　*/
