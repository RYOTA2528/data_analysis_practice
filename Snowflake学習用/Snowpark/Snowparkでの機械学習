#パッケージのトグルを開き、検索でsnowflake-ml-python,snowflake-snowpark-pythonのインポート
# 左記にenvironment.ymlができる(ここに上記記載される)
#使用する元データ(CUSTOMER_DATA_180)にはID, AGE,GENDER,ANNUAL_INCOME(年収), CHURN列(解約なら1,0)がある。AGE,GENDER,ANNUAL_INCOMEを使い予測
#XGブーストによる機械学習

## Sessionの確立
from snoflake.snowpark.cntext import get_active_session
session = get_active_session()

df = session.table("CUSTOMER_DATA_1808")
df
type(df) # snowflakeであることがわかる

## 学習データとテストデータにわける
train_df, test_df = df.random_split([0.6, 0.2], seed = 2) #seed=2：乱数の初期化値を指定して、毎回同じ分割結果にするため。
print(train_df.count(), test_df.count())


## インポート

#Snowflake Machine Learning（Snowpark ML） で 欠損値（NaN/null）の補完処理（imputation） を行うためのモジュール
from snowflake.ml.modeling.impute import SimpleImputer
#StanderdScaler:特徴量の**標準化（平均0、標準偏差1）**を行う前処理クラス
#OrdinalEnocoder:カテゴリ変数（文字列など）を、順序付きの整数（0, 1, 2, ...）に変換する前処理クラス
from snowflake.ml.modeling.preprocessing import StanderdScaler, OrdinalEnocoder
# XGboostの使用
from snowflake.ml.modeling.xgboost import XGBClassifier
# 複数の前処理や学習処理を一連のステップとしてまとめる
from snowflake.ml.modeling pipeline import Pipeline


## 数値データ（age, annual_income）に欠損値補完を行うための SimpleImputer を作成
numeric_imputer = SimpleImputer(
    strategy="median",  # 欠損値の補完方法として「中央値（median）」を使用
    input_cols=["age", "annual_income"],  # 補完対象の列（欠損がある可能性のある数値列）
    output_cols=["age", "annual_income"],  # 補完後に出力される列名（上書きする形）
    passthrough_cols=["gender", "churn"],  # 補完しないが、そのままパイプラインに通す列
    drop_input_cols=True  # 入力列を削除（output_cols と同じなら上書きの意味）
)

## カテゴリ変数（文字列）の欠損値を補完するための SimpleImputer を作成
categorical_imputer = SimpleImputer(
    strategy="most_frequent",  # 欠損値を「最も頻繁に出現する値（最頻値）」で補完
    input_cols=["gender"],     # 補完の対象となる列（ここでは gender）
    output_cols=["gender"],    # 補完後の出力列（同名なので上書き）
    passthrough_cols=["age", "anuual_income", "churn"],  # 補完せずにそのまま通す列（後続処理のために保持）
    drop_input_cols=True       # 元の gender 列を削除し、output_cols で置き換える
)

## カテゴリ列 "gender" を数値（順序付きの整数）に変換するエンコーダ
encoder = OrdinalEncoder(
    input_cols=["gender"],                      # 変換したい元のカテゴリ列（例："Male", "Female"）
    output_cols=["gender_code"],                # エンコード後の新しい列名（整数が入る）
    passthrough_cols=["age", "annual_income", "churn"],  # この3列は変換せずにそのまま残す（他の処理で使うため）
    drop_input_cols=True                        # 元の "gender" 列は削除し、"gender_code" に置き換える
)

## age と annual_income のスケールを標準化する（平均0、標準偏差1に変換）
scaler = StandardScaler(
    input_cols=["age", "annual_income"],            # スケーリング対象の数値列
    output_cols=["age", "annual_income"],           # 出力列（同じ列名を使うと上書きになる）
    passthrough_cols=["gender_code", "churn"],      # スケーリングせずそのまま保持する列
    drop_input_cols=True                            # 元の age, annual_income 列は削除し、出力列で置き換える
)

## モデリングを実施(パイプラインの実施)
# XGBoost分類器の定義：予測対象は "churn"（解約かどうか）
xgb = XGBClassifier(
    input_cols=["age", "annual_income", "gender_code"],  # モデルに使う特徴量
    label_cols=["churn"],                                # 正解ラベル列（ターゲット）
    output_cols=["predicted_churn"]                      # モデルが出力する予測列の名前
)

pipleine = Pipeline( steps = [
    ("num_imputer", numeric_imputer),
    ("cat_imputer", categorical_imputer),
    ("encoder", enccoder),
    ("scaler", scaler),
    ("classifier, xgb")
])

model = pipeline.fit(train_df)


## 予測の実施、結果の確認
pred = model.predict(test_df) #学習済みモデルでテストデータに対して予測を行う処理
pred.select("ID", "AGE", "GENDER_CODE", "ANNUAL_INCOME", "CHURN", "PREDICT_CHRUN")
(※pred.show()のほうがみやすい)




